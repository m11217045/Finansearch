"""
ä¸»ç¨‹å¼ - S&P 500 åƒ¹å€¼æŠ•è³‡è‚¡ç¥¨ç¯©é¸èˆ‡åˆ†æç³»çµ±
"""

import pandas as pd
import logging
import os
from datetime import datetime
import json

# å°å…¥è‡ªè¨‚æ¨¡çµ„
from src.utils import setup_logging, load_env_variables, create_output_directory, save_dataframe
from src.data_fetcher import SP500DataFetcher
from src.screener import ValueScreener
from src.enhanced_analyzer import EnhancedStockAnalyzer  # ä½¿ç”¨å¢å¼·ç‰ˆåˆ†æå™¨
from config.settings import OUTPUT_SETTINGS


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    # è¨­ç½®æ—¥èªŒ
    setup_logging()
    logger = logging.getLogger(__name__)
    
    logger.info("=" * 60)
    logger.info("S&P 500 åƒ¹å€¼æŠ•è³‡è‚¡ç¥¨ç¯©é¸èˆ‡åˆ†æç³»çµ±å•Ÿå‹•")
    logger.info("=" * 60)
    
    try:
        # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
        env_vars = load_env_variables()
        max_stocks_to_analyze = env_vars.get('max_stocks', OUTPUT_SETTINGS['max_stocks_to_analyze'])
        
        # å»ºç«‹è¼¸å‡ºç›®éŒ„
        output_dir = create_output_directory()
        logger.info(f"è¼¸å‡ºç›®éŒ„: {output_dir}")
        
        # éšæ®µ 1: ç²å– S&P 500 æˆåˆ†è‚¡æ•¸æ“š
        logger.info("\néšæ®µ 1: ç²å– S&P 500 æˆåˆ†è‚¡æ•¸æ“š")
        logger.info("-" * 40)
        
        fetcher = SP500DataFetcher()
        
        # ç²å–è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨
        tickers = fetcher.get_sp500_tickers()
        fetcher.save_tickers_to_csv(f"{output_dir}/sp500_tickers.csv")
        
        # æ‰¹é‡ç²å–è‚¡ç¥¨æ•¸æ“šï¼ˆé™åˆ¶æ•¸é‡ä»¥ç¯€çœæ™‚é–“ï¼‰
        logger.info(f"ç²å–å‰ {min(100, len(tickers))} æ”¯è‚¡ç¥¨çš„è²¡å‹™æ•¸æ“š...")
        raw_data = fetcher.batch_fetch_stock_data(tickers, max_stocks=100)
        
        if raw_data.empty:
            logger.error("ç„¡æ³•ç²å–è‚¡ç¥¨æ•¸æ“šï¼Œç¨‹å¼çµæŸ")
            return
        
        # ä¿å­˜åŸå§‹æ•¸æ“š
        save_dataframe(raw_data, f"{output_dir}/raw_stock_data", ['csv', 'json'])
        logger.info(f"åŸå§‹æ•¸æ“šå·²ä¿å­˜ï¼ŒåŒ…å« {len(raw_data)} æ”¯è‚¡ç¥¨")
        
        # éšæ®µ 2: åƒ¹å€¼æŠ•è³‡æ’ååˆ†æ
        logger.info("\néšæ®µ 2: åƒ¹å€¼æŠ•è³‡æ’ååˆ†æ")
        logger.info("-" * 40)
        
        screener = ValueScreener()
        
        # ç›´æ¥ç²å–è¢«ä½ä¼°ç¨‹åº¦å‰10åçš„è‚¡ç¥¨
        top_undervalued_stocks = screener.get_top_undervalued_stocks(raw_data, top_n=10)
        
        if top_undervalued_stocks.empty:
            logger.warning("æ²’æœ‰æ‰¾åˆ°å¯æ’åçš„è‚¡ç¥¨")
            return
        
        # ä¿å­˜è¢«ä½ä¼°è‚¡ç¥¨æ’åçµæœ
        save_dataframe(top_undervalued_stocks, f"{output_dir}/top_undervalued_stocks", ['csv', 'json'])
        logger.info(f"è¢«ä½ä¼°è‚¡ç¥¨æ’åå·²ä¿å­˜ï¼Œå‰10åè‚¡ç¥¨å¦‚ä¸‹ï¼š")
        
        # é¡¯ç¤ºå‰10åç°¡è¦ä¿¡æ¯
        for _, stock in top_undervalued_stocks.head(10).iterrows():
            logger.info(f"  ç¬¬{int(stock['value_rank'])}å: {stock['ticker']} ({stock.get('company_name', 'N/A')}) - è©•åˆ†: {stock['value_score']:.1f}")
        
        # ä½¿ç”¨æ’åçµæœé€²è¡Œå¾ŒçºŒåˆ†æ
        screened_data = top_undervalued_stocks
        
        # å»ºç«‹ç¯©é¸æ‘˜è¦
        screening_summary = screener.screening_results
        with open(f"{output_dir}/screening_summary.json", 'w', encoding='utf-8') as f:
            json.dump(screening_summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åƒ¹å€¼æŠ•è³‡æ’åå®Œæˆï¼Œç²å¾—å‰ {len(screened_data)} æ”¯è¢«ä½ä¼°è‚¡ç¥¨")
        
        # é¡¯ç¤ºå‰ 5 åè‚¡ç¥¨
        logger.info("\nå‰ 5 åè¢«ä½ä¼°è‚¡ç¥¨:")
        for i, row in screened_data.head().iterrows():
            logger.info(f"{int(row['value_rank'])}. {row['ticker']} ({row.get('company_name', 'N/A')}) - è©•åˆ†: {row['value_score']:.2f}")
        
        # éšæ®µ 3: å¢å¼·ç‰ˆ AI ç¶œåˆåˆ†æ
        logger.info("\néšæ®µ 3: å¢å¼·ç‰ˆ AI ç¶œåˆåˆ†æ")
        logger.info("-" * 40)
        
        try:
            analyzer = EnhancedStockAnalyzer()
            
            # å°‡ DataFrame è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨
            stock_list = screened_data.to_dict('records')
            
            # åŸ·è¡Œç¶œåˆåˆ†æï¼ˆåŒ…å«æ–°èã€æƒ…ç·’ã€æŠ€è¡“é¢ï¼‰
            logger.info("é–‹å§‹åŸ·è¡Œå¤šç¶­åº¦ç¶œåˆåˆ†æ...")
            analysis_results = analyzer.batch_analyze_stocks(stock_list, max_analysis=min(5, len(screened_data)))
            
            # ä¿å­˜åˆ†æçµæœ
            analyzer.save_analysis_results(analysis_results, f"{output_dir}/enhanced_analysis")
            
            # é¡¯ç¤ºåˆ†ææ‘˜è¦
            successful_count = analysis_results.get('successful_analyses', 0)
            total_count = analysis_results.get('total_stocks_requested', 0)
            logger.info(f"ç¶œåˆåˆ†æå®Œæˆ: {successful_count}/{total_count} æ”¯è‚¡ç¥¨åˆ†ææˆåŠŸ")
            
            # é¡¯ç¤ºå‰3åè‚¡ç¥¨çš„æŠ•è³‡å»ºè­°
            if analysis_results.get('analysis_results'):
                logger.info("\nğŸ“Š å‰3åè‚¡ç¥¨æŠ•è³‡å»ºè­°:")
                count = 0
                for ticker, result in analysis_results['analysis_results'].items():
                    if 'error' not in result and count < 3:
                        logger.info(f"  {ticker}: {result.get('investment_recommendation', 'N/A')} "
                                  f"(ç¶œåˆè©•åˆ†: {result.get('overall_score', 0):.1f})")
                        logger.info(f"    é¢¨éšªç­‰ç´š: {result.get('risk_assessment', {}).get('overall_risk', 'N/A')}")
                        logger.info(f"    æ–°èæƒ…ç·’: {result.get('news_sentiment_analysis', {}).get('sentiment', 'N/A')}")
                        count += 1
            
            # ç”ŸæˆæŠ•è³‡å ±å‘Š
            investment_reports = []
            if analysis_results.get('analysis_results'):
                for ticker, result in analysis_results['analysis_results'].items():
                    if 'error' not in result:
                        investment_reports.append({
                            'ticker': ticker,
                            'recommendation': result.get('investment_recommendation', ''),
                            'overall_score': result.get('overall_score', 0),
                            'risk_level': result.get('risk_assessment', {}).get('overall_risk', ''),
                            'sentiment': result.get('news_sentiment_analysis', {}).get('sentiment', '')
                        })
            
            # ä¿å­˜æŠ•è³‡å ±å‘Š
            if investment_reports:
                reports_df = pd.DataFrame(investment_reports)
                save_dataframe(reports_df, f"{output_dir}/investment_reports", ['csv'])
            
            logger.info(f"å¢å¼·ç‰ˆåˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(investment_reports)} ä»½ç¶œåˆæŠ•è³‡å ±å‘Š")
            
        except Exception as e:
            logger.error(f"å¢å¼·ç‰ˆåˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            logger.info("å°‡è·³é AI åˆ†æï¼Œåƒ…æä¾›ç¯©é¸çµæœ")
        
        # éšæ®µ 4: ç”Ÿæˆæœ€çµ‚å ±å‘Š
        logger.info("\néšæ®µ 4: ç”Ÿæˆæœ€çµ‚å ±å‘Š")
        logger.info("-" * 40)
        
        # å»ºç«‹ç°¡è¦å ±å‘Š
        create_summary_report(screened_data, output_dir)
        
        # ç”Ÿæˆè¡Œæ¥­åˆ†æ
        if 'sector' in screened_data.columns:
            sector_analysis = screener.generate_sector_analysis(screened_data)
            if sector_analysis is not None and not sector_analysis.empty:
                save_dataframe(sector_analysis, f"{output_dir}/sector_analysis", ['csv'])
        
        logger.info(f"\nç¨‹å¼åŸ·è¡Œå®Œæˆï¼")
        logger.info(f"æ‰€æœ‰çµæœå·²ä¿å­˜åˆ°: {output_dir}")
        logger.info("=" * 60)
        
        # é¡¯ç¤ºè¼¸å‡ºæª”æ¡ˆåˆ—è¡¨
        output_files = [f for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]
        logger.info("è¼¸å‡ºæª”æ¡ˆ:")
        for file in sorted(output_files):
            logger.info(f"  - {file}")
        
    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise


def create_summary_report(top_stocks: pd.DataFrame, output_dir: str) -> None:
    """å»ºç«‹ç°¡è¦æ‘˜è¦å ±å‘Š"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report = f"""
S&P 500 åƒ¹å€¼æŠ•è³‡è‚¡ç¥¨ç¯©é¸çµæœæ‘˜è¦
ç”Ÿæˆæ™‚é–“: {timestamp}

ç¯©é¸æ¨™æº–:
- æœ¬ç›Šæ¯” (P/E) â‰¤ 20
- å¸‚æ·¨ç‡ (P/B) â‰¤ 2
- è‚¡æ¯æ®–åˆ©ç‡ â‰¥ 3%
- å‚µå‹™æ¬Šç›Šæ¯” â‰¤ 1.5
- è‡ªç”±ç¾é‡‘æµ > 0

å‰ {min(10, len(top_stocks))} åæ¨è–¦è‚¡ç¥¨:
"""
    
    for i, row in top_stocks.head(10).iterrows():
        pe = f"{row['trailing_pe']:.2f}" if pd.notna(row['trailing_pe']) else "N/A"
        pb = f"{row['price_to_book']:.2f}" if pd.notna(row['price_to_book']) else "N/A"
        div_yield = f"{row['dividend_yield']*100:.2f}%" if pd.notna(row['dividend_yield']) else "N/A"
        score = f"{row['total_value_score']:.2f}" if pd.notna(row['total_value_score']) else "N/A"
        
        report += f"""
{row['rank']}. {row['ticker']} - {row.get('company_name', 'Unknown')}
   è¡Œæ¥­: {row.get('sector', 'Unknown')}
   æœ¬ç›Šæ¯”: {pe} | å¸‚æ·¨ç‡: {pb} | è‚¡æ¯ç‡: {div_yield}
   åƒ¹å€¼è©•åˆ†: {score} | è©•ç­‰: {row.get('value_rating', 'N/A')}
"""
    
    report += f"""

å…è²¬è²æ˜:
æœ¬å ±å‘Šåƒ…ä¾›æ•™è‚²å’Œåƒè€ƒç”¨é€”ï¼Œä¸æ§‹æˆæŠ•è³‡å»ºè­°ã€‚
æŠ•è³‡å‰è«‹è«®è©¢å°ˆæ¥­è²¡å‹™é¡§å•ä¸¦é€²è¡Œå……åˆ†çš„ç›¡è·èª¿æŸ¥ã€‚
è‚¡ç¥¨æŠ•è³‡æ¶‰åŠé¢¨éšªï¼Œéå»è¡¨ç¾ä¸ä»£è¡¨æœªä¾†çµæœã€‚
"""
    
    with open(f"{output_dir}/summary_report.txt", 'w', encoding='utf-8') as f:
        f.write(report)


if __name__ == "__main__":
    main()
